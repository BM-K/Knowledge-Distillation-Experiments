# Knowledge-Distillation-Experiments
- 지식 증류 실험: <br>
  - Distilling the Knowledge in a Neural Network (1) <br>
  - Improved Knowledge Distillation via Teacher Assistant (2) <br>

# LSTM (1) <br>
- Data set: '한국어 악성댓글 탐지 데이터셋'
<img src = "https://user-images.githubusercontent.com/55969260/113986832-a966e880-9888-11eb-826a-cf70b85ad685.png">
<br>

# Transformer (1), (2)<br>
- Data set: '네이버 영화평 감정분석 데이터셋'
<img src = "https://user-images.githubusercontent.com/55969260/114114777-82f28d00-991c-11eb-8d6f-4c285e483be4.png"> 
<br>
